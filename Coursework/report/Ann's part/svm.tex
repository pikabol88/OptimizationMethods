\begin{document}
\subsection{Краткое описание метода [1, 57-63]}
\textbf{Метод опорный векторов (SVM)} разработан в 60-е годы коллективом советских математиков под руководством В.Н.Вапника и рассчитан на классифицирование объектов по двум классам.

Пусть имеется обучающая выборка $G^l,|G^l|=n$, заданная множеством пар прецедентов $(\bar{x}_i,y_i),i=\overline{1,n},\bar{x}_i\in\mathbb{R}^m,y_i\in\{-1,+1\}$.

\subsubsection{Разделяющая гиперплоскость}
Множество $F_{SVM}$, из которого выбираются решающие функции по методу опорных векторов, образовано функциями вида:
\begin{equation}
    f(\bar{x})=sign(<\bar{w},\bar{x}>+w_0),
\end{equation}
где $<,>$ - скалярное произведение векторов, $\bar{w}$ - ортонормированный вектор к разделяющей классы гиперплоскости, $w_0$ - вспомогательный параметр (сдвиг гиперплоскости).

Так как любая гиперплоскость может быть задана в виде $<\bar{w},\bar{x}>+w_0=0$, то объекты с $f(\bar{x})\leq-1$ попадут в один класс, а объекты с $f(\bar{x})\geq+1$ - в другой.

\textit{Базовая идея метода:} найти такие $\bar{w},w_0$, которые максимизируют расстояние между классами, что приводит к более уверенной классификации объектов. При этом считается, что нормировка параметров уже произведена.

То есть условие $-1<<\bar{w},\bar{x}>+w_0<+1$ задает полосу, разделяющую классы. При этом ни одна точка из множества $X^l$ не должна лежать внутри полосы, а границами полосы являются две параллельные гиперплоскости, проходящие через точки (объекты), ближайшие к разделяющей гиперплоскости, которая находится по середине данной полосы. И объекты, через которые проходят границы полосы, называются \textit{опорными векторами}.

Ширина полосы будет равна $\frac{2}{||w||}$, и она максимальна, когда норма вектора $w$ минимальна [4, 3].

\subsubsection{Линейно разделимый случай выборки}
Проблема нахождения максимума расстояния сводится к нахождению минимума $||\bar{w}||^2$, которая является стандартной прямой задачей квадратичного программирования:
\begin{equation}
    \left\{
    \begin{array}{ll}
        \frac{1}{2}||\bar{w}||^2\rightarrow\min\\
        y_i<\bar{w},\bar{x}_i+w_0>\geq1\\
    \end{array}
    \right.
\end{equation}
и решается методом множителей Лагранжа.

Задача квадратичного программирования, содержащая только двойственные переменные метода множителей Лагранжа $\lambda_i$, имеет вид:
\begin{equation}
    \left\{
    \begin{array}{ll}
        -\sum_{i=1}^n{\lambda_i}+\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n{\lambda_i\lambda_jy_iy_j<\bar{x}_i,\bar{x}_j>}\rightarrow\min_{\lambda}\\
        \sum_{i=1}^n{\lambda_iy_i}=0\\
        \lambda_i\geq0\\
    \end{array}
    \right.
\end{equation}

В результате рещающая функция приобретает вид:
\begin{equation}
    f(\bar{x})=sign(\sum_{i=1}^n{\lambda_iy_i<\bar{x}_i,\bar{x}>+w_0)},
\end{equation}
где параметр $w_0=med\{<\bar{w},\bar{x}_i>-y_i\}, \lambda_i\not=0$.

\subsubsection{Случай линейно неразделимой выборки}
Вышеуказанные рассуждения справедливы для линейно разделимой обучающей выборки. Но на практике встречаются случае линейной неразделимости и решающей функции позволяют допускать ошибки на обучающей выборке, но эти ошибки минимизируют и используют управляющую константу $C$ как компромисс между максимизацией ширины разделяющей полосы и минимизацией суммарной ошибки $\xi_i\geq0$. Тогда вводят ограничение сверху $0\leq\lambda_i\leq{C}$ и такой алгоритм называют SVM с "мягким зазором" (soft-margin SVM), иначе имеется "жесткий" зазор.

Тогда прямая задача квадратичного программирования имеет вид:
\begin{equation}
    \left\{
    \begin{array}{ll}
        \frac{1}{2}||\bar{w}||^2+C\sum_{i=1}^n{\xi_i}\rightarrow\min\\
        y_i<\bar{w},\bar{x}_i+w_0>\geq1-\xi_i\\
    \end{array}
    \right.
\end{equation}

А двойственная задача квадратичного программирования находится как:
\begin{equation}
    \left\{
    \begin{array}{ll}
        -\sum_{i=1}^n{\lambda_i}+\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n{\lambda_i\lambda_jy_iy_j<\bar{x}_i,\bar{x}_j>}\rightarrow\min_{\lambda}\\
        \sum_{i=1}^n{\lambda_iy_i}=0\\
        0\leq\lambda_i\leq{C}\\
    \end{array}
    \right.
\end{equation}

\subsubsection{Ядра}
Если признаки $x_i$ заданы в виде функции $\theta(x_i)$, то решающая функция строится аналогично. Тогда функция $K(u,v)=<\theta(u),\theta(v)>$ - \textit{ядро}, если она симметрична и положительно определена. Для решения практических задач классификации изображений по атрибуту "пол" используют RFB-ядро вида:
\begin{equation}
    K(\bar{x}_i,\bar{x})=\exp{(-\gamma||\bar{x}_i-\bar{x}||^2)},
\end{equation}
вычисляющее оценку близости вектора $\bar{x}$ к опорному вектору $\bar{x}_i$, где $\gamma$ - некоторый параметр.

Тогда решающая функция принимает вид:
\begin{equation}
    f(\bar{x})=sign(\sum_{i=1}^n{\lambda_iy_iK(\bar{x}_i,\bar{x})+w_0)},
\end{equation}

\subsection{Множественная классификация по атрибуту}
Классификация по атрибуту "раса" является типичной задачей множественной классификации. Для решения такой задачи используется подход \textit{"один против всех"}, реализующий сведение задачи множественной классификации к последовательному применению бинарных классификаторов. В рамках данного подхода строится бинарное дерево решающих функций $f\in{F}$, каждая из которых выделяет только один класс объектов.

В случае классификации по атрибуту "раса" на первом шаге объекты разделяются решающей фукцией $f_1$ на два класса: "европеоиды" и "все остальные". Если объект не попал в класс "европеоиды", то на втором шаге другая решающая функция $f_2$ производит разделение на класс "монголоиды" и "все остальные" ("негроиды").

Данный подход позволяет использовать большую часть разработок в области бинарной классификации для решения задач множественной классификации.

Общая формулировка прямой задачи имеет вид:
\begin{equation}
    \left\{
    \begin{array}{ll}
        \frac{1}{2}||\bar{w}_k||^2+C\sum_{i=1}^n{\xi_i}\rightarrow\min\\
        <\bar{w}_{k_i},\bar{x}_i>-<\bar{w}_k,\bar{x}_i>\geq1-\delta k_i,y-\xi_i\\
    \end{array}
    \right.
\end{equation}
где $k\in K$ - номера классов атрибута.

А двойственная задача квадратичного программирования находится как:
\begin{equation}
    \left\{
    \begin{array}{ll}
        \frac{1}{2}\sum_{k\in K}\sum_{i=1}^n\sum_{j=1}^n{\lambda_{k_i}\lambda_{k_j}<\bar{x}_i,\bar{x}_j>}+\sum_{k\in K}\sum_{i=1}^n(1-\delta k_i,k)\lambda_{k_i}\rightarrow\min_{\lambda}\\
        0\leq\lambda_{k_i}\leq{C}\\
    \end{array}
    \right.
\end{equation}

В результате рещающая функция приобретает вид:
\begin{equation}
    f(\bar{x})=argmax_{k\in K}(<\bar{w}_k,\bar{x}>+w_{0_k}),
\end{equation}
\end{document}
